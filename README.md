# AzureDataLakeTour
* Exploration of a basic azure data lake setting up process and its use (cloud environment: azure)
* For instance, we will try:
    1. To load data from different sources (local machine, azure blob storages, api, etc.). From both the web environment and azcopy tool.
    2. Azure datafactory, which is Azure's own ETL/ELT, to make some basis pipelines.

## Context
The main purpose here is to create some resources on azure.
The main goal is to create them using azure web interface but we will leverage that stage to do it using terraform too (hence that repo).
Thus we will be abble to manage more easily the life cycle of the created resources as well as their protential cost (which should not be huge anyway)

## Main tips
Basically, this repo is not intended to be reused by anyone except me.
However, if ever you want to use it for your own. Below are some usefull tips & requirements.
1. Before running this repo, make sure you get an azure account and sign in to it using your own credentials.<br>
You have to do so via the command line, so that project's scripts can run, but also on the azure web portal directly in order to check results (at least!).
    * Requirements (third party tools to be installed on your machine):
        * Azure cli
        * Azure azcopy
        * Terraform
        * Any tool you like to run bash scripts (within the project directories at least !)

2. When running any terraform command like `terraform init`, `terraform plan`, `terraform apply` and so on, place yourself in the `DataLakeDeployment` directory

3. Check `terraform.tfvars` file to customize resource names.
    * If ever you want to customize deeper then feel free to explore and change the `2_main.tf` file content.

4. Two additional yet optional scripts are proposed in `DataLakeDeployment` directory.
    * `.terraformDestroyAll.sh` : To perform recursive destroy so that files being stored in the containers do not stop destroy process (See description in the script).
    * `.reset.sh` : To be executed to remove all files that have been automatically generated by terraforms. Hence the script's name...<br>
    (Caution: To be executed only after a terraform destroy call.)
    * To run these utility scripts:
        1. place yourself where the files are located (i.e. `~/AzureDataLakeTour/DataLakeDeployment/`)
        2. run `bash ./<script name>`

## Resources location
A directory called 'Resources' is on the project root. Its purpose is to collect all testing files used to test the data lake and data factory features.